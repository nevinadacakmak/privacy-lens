{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc23ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# dp_training.ipynb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "# 1. Load dataset (AG News)\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "train_dataset = dataset[\"train\"].with_format(\"torch\")\n",
    "test_dataset = dataset[\"test\"].with_format(\"torch\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 2. Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 3. Attach Opacus Privacy Engine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=1.0,   # controls noise level\n",
    "    max_grad_norm=1.0,      # gradient clipping\n",
    ")\n",
    "\n",
    "# 4. Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(2):  # fewer epochs since DP training is slower\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epsilon, best_alpha = privacy_engine.get_epsilon(delta=1e-5)\n",
    "    print(f\"Epoch {epoch+1} | DP budget (ε): {epsilon:.2f}, α: {best_alpha}\")\n",
    "\n",
    "# 5. Evaluate accuracy\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy with DP: {100*correct/total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
